{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Настройка окружения"
      ],
      "metadata": {
        "id": "Fdabi9xlQsLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Монтирование Google-Диска"
      ],
      "metadata": {
        "id": "Hluu8gkqQvZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "roFEfbkwQbz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563cb40e-3b4b-4688-bd2a-291f945a4f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установка зависимостей"
      ],
      "metadata": {
        "id": "qeryuBn_QzE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install openpyxl deepdiff pillow nltk tqdm langchain_community langgraph_sdk langchain_openai langgraph langchain_core langsmith ftfy openai httpx==0.27.2"
      ],
      "metadata": {
        "id": "e5EFMBZOQiEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Импорты"
      ],
      "metadata": {
        "id": "czI6T2ZWdxmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from deepdiff import DeepDiff\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "import operator\n",
        "\n",
        "import uuid\n",
        "\n",
        "import asyncio\n",
        "\n",
        "import builtins\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langchain_core.runnables.config import RunnableConfig\n",
        "from langgraph.constants import Send\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "\n",
        "from jinja2 import Template\n",
        "\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import Optional, Literal, Union\n",
        "import datetime\n",
        "\n",
        "from typing import Annotated, Any, Optional, List, Literal, Dict, Type, Tuple\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import time\n",
        "\n",
        "import yaml\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "rgfgUSBldu-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Переменные окружения"
      ],
      "metadata": {
        "id": "umCTYRQ4Q3q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Укажите базовый URL для API, если используете альтернативный хост\n",
        "os.environ['API_BASE_URL'] = None  # Например, \"https://api.fireworks.ai/inference/v1\"\n",
        "\n",
        "# Укажите путь к конфигу с промптами\n",
        "os.environ['PROMPT_CONFIG_PATH'] = \"../../data/configs/prompts.yaml\"\""
      ],
      "metadata": {
        "id": "5AMlxPm_Qrpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt-4.1\" # Укажите навзвание модели\n",
        "main_dir = \"drive/MyDrive/LLM_coding_challenge\" # Рабочая директория\n",
        "\n",
        "# Поддиректории с исходными и целевыми данными\n",
        "init_data_dir = os.path.join(main_dir, \"initial_data\")\n",
        "important_data_dir = os.path.join(main_dir, \"important_data\")"
      ],
      "metadata": {
        "id": "oqiB8ZRtccPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация модели через API. Не забудьте задать OPENAI_API_KEY в переменных окружения.\n",
        "model = ChatOpenAI(\n",
        "    model=model_name,\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    # openai_api_base=os.getenv(\"API_BASE_URL\"), # Раскомментируйте, если нужен нестандартный endpoint\n",
        "    max_retries=30,\n",
        "    timeout=90,\n",
        ")"
      ],
      "metadata": {
        "id": "in167HHU1hKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Конфиги"
      ],
      "metadata": {
        "id": "nG_Hk5GAvea4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_prompts_config():\n",
        "    \"\"\"\n",
        "    Загружает YAML-конфиг с промптами из файла, путь к которому задан через переменную окружения PROMPT_CONFIG_PATH\n",
        "    \"\"\"\n",
        "    config_path = Path(os.environ[\"PROMPT_CONFIG_PATH\"])\n",
        "    if not config_path.exists():\n",
        "        raise FileNotFoundError(f\"Файл промптов не найден по пути {config_path}\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "PROMPTS_CONFIG = load_prompts_config()"
      ],
      "metadata": {
        "id": "1PFTM-Qkvfub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Исследование и обработка данных"
      ],
      "metadata": {
        "id": "INLKrJDCyPEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = os.path.join(init_data_dir, \"Примеры внесения информации из присылаемых сообщений в таблицы.xlsx\")\n",
        "\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "S8GYKvwZyR2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_structured_data_from_df(df):\n",
        "    structured_data = []\n",
        "    marker_text = \"Пример сообщения от агронома\"\n",
        "    message_column_index = 0\n",
        "\n",
        "    i = 0\n",
        "    while i < len(df):\n",
        "        # Проверяем, является ли значение в ячейке строкой и начинается ли оно с маркерного текста\n",
        "        cell_value = df.iloc[i, message_column_index]\n",
        "        if isinstance(cell_value, str) and cell_value.strip().startswith(marker_text):\n",
        "\n",
        "            # Нашли маркер. Сообщение агронома в следующей строке, в том же столбце\n",
        "            if i + 1 < len(df):\n",
        "                agronomist_message = df.iloc[i + 1, message_column_index]\n",
        "            else:\n",
        "                # Если маркер в последней строке, сообщения нет\n",
        "                agronomist_message = None\n",
        "                i += 1\n",
        "                continue # Переходим к следующей итерации внешнего цикла\n",
        "\n",
        "            # Заголовки таблицы находятся через одну строку от маркера (i+2)\n",
        "            if i + 2 < len(df):\n",
        "                header_row = df.iloc[i + 2]\n",
        "                # Убираем возможные NaN из заголовков, заменяя их на что-то осмысленное или пустую строку\n",
        "                table_headers = [str(h) if pd.notna(h) else f'Unnamed_{col_idx}' for col_idx, h in enumerate(header_row)]\n",
        "            else:\n",
        "                # Если нет места для заголовков, пропускаем этот блок\n",
        "                i += 1 # Пропускаем строку маркера для следующей итерации\n",
        "                continue\n",
        "\n",
        "            # Данные таблицы начинаются со строки i + 3\n",
        "            data_start_index = i + 3\n",
        "            current_table_rows = []\n",
        "            j = data_start_index\n",
        "\n",
        "            while j < len(df):\n",
        "                current_row = df.iloc[j]\n",
        "                # Проверяем, состоит ли вся строка из NaN\n",
        "                if current_row.isnull().all():\n",
        "                    # Нашли конец таблицы (строка со всеми NaN)\n",
        "                    break\n",
        "                else:\n",
        "                    # Это строка данных, добавляем её в список\n",
        "                    current_table_rows.append(current_row.tolist()) # Преобразуем в список для DataFrame\n",
        "                    j += 1\n",
        "\n",
        "            # Если мы собрали какие-то строки для таблицы\n",
        "            if current_table_rows:\n",
        "                # Создаем DataFrame из собранных строк с правильными заголовками\n",
        "                table_df = pd.DataFrame(current_table_rows, columns=table_headers)\n",
        "                table_df = table_df.drop(columns=[\"Unnamed_0\"])\n",
        "                # Добавляем результат (сообщение и DataFrame таблицы) в общий список\n",
        "                structured_data.append({\n",
        "                    \"agronomist_message\": agronomist_message,\n",
        "                    \"data_table\": table_df\n",
        "                })\n",
        "            else:\n",
        "                # Если данных для таблицы не найдено после заголовков (например, сразу NaN строка)\n",
        "                # Можно добавить запись с пустой таблицей или пропустить\n",
        "                structured_data.append({\n",
        "                    \"agronomist_message\": agronomist_message,\n",
        "                    \"data_table\": pd.DataFrame(columns=table_headers) # Пустой DataFrame с заголовками\n",
        "                })\n",
        "\n",
        "            # Перемещаем основной индекс i за пределы только что обработанного блока\n",
        "            i = j + 1 # Начинаем следующий поиск со строки после найденной NaN-строки\n",
        "\n",
        "        else:\n",
        "            # Если текущая строка не маркер, просто переходим к следующей\n",
        "            i += 1\n",
        "\n",
        "    return structured_data"
      ],
      "metadata": {
        "id": "ECw0--0pyUHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_data = extract_structured_data_from_df(df)\n",
        "\n",
        "print(f\"\\nНайдено и обработано {len(structured_data)} блоков сообщений.\")\n",
        "structured_data[0]['data_table']"
      ],
      "metadata": {
        "id": "fCjYWgEUyVGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Output"
      ],
      "metadata": {
        "id": "jMt1oZ7MqQr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Schemas\n",
        "Набор допустимых значений для полей\n",
        "\n",
        "Используется при декодировании ответа модели с ограничениями (constrained decoding).\n",
        "\n",
        "Модель генерирует ответ в формате, строго соответствующем Pydantic-схеме,\n",
        "и не может выйти за рамки заданного множества значений."
      ],
      "metadata": {
        "id": "3fdzjTvM1qi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "department_set = tuple(['1', '10', '11', '12', '14', '16', '17', '18',\n",
        "                                 '19', '20', '3', '4', '5', '6', '7', '9',\n",
        "                                 'АО Кропоткинское', 'АОР', 'Восход', 'Рассвет','Кавказ',\n",
        "                                 'Колхоз Прогресс', 'Мир', 'СП Коломейцево',\n",
        "                                 'Север', 'ТСК', 'Центр', 'Юг'])\n",
        "\n",
        "operation_set = tuple(['1-я междурядная культивация', '2-я междурядная культивация',\n",
        "                       'Боронование довсходовое', 'Внесение минеральных удобрений',\n",
        "                       'Выравнивание зяби', '2-е Выравнивание зяби',\n",
        "                       'Гербицидная обработка', '1 Гербицидная обработка',\n",
        "                       '2 Гербицидная обработка', '3 Гербицидная обработка',\n",
        "                       '4 Гербицидная обработка', 'Дискование', 'Дискование 2-е', 'Дискование 3-е',\n",
        "                       'Инсектицидная обработка', 'Культивация', 'Пахота',\n",
        "                       'Подкормка', '2-я подкормка', 'Предпосевная культивация', 'Прикатывание посевов',\n",
        "                       'Сев', 'Сплошная культивация', 'Уборка', 'Функицидная обработка',\n",
        "                       'Чизлевание'])\n",
        "\n",
        "crop_set = tuple(['Вика+Тритикале', 'Горох на зерно', 'Горох товарный', 'Гуар',\n",
        "                  'Конопля', 'Кориандр', 'Кукуруза кормовая', 'Кукуруза семенная',\n",
        "                  'Кукуруза товарная', 'Люцерна', 'Многолетние злаковые травы',\n",
        "                  'Многолетние травы текущего года','Овес', 'Подсолнечник кондитерский',\n",
        "                  'Подсолнечник семенной','Подсолнечник товарный', 'Просо','Пшеница озимая на зеленый корм',\n",
        "                  'Пшеница озимая семенная', 'Пшеница озимая товарная', 'Рапс озимый',\n",
        "                  'Рапс яровой', 'Свекла сахарная', 'Сорго', 'Сорго кормовой',\n",
        "                  'Сорго-суданковый гибрид', 'Соя семенная', 'Соя товарная',\n",
        "                  'Чистый пар', 'Чумиза', 'Ячмень озимый', 'Ячмень озимый семенной'])"
      ],
      "metadata": {
        "id": "PRDFkLlkdPSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_base_field_works_schemas(\n",
        "    department_set: Tuple[str],\n",
        "    operation_set: Tuple[str],\n",
        "    crop_set: Tuple[str],\n",
        "):\n",
        "    class FieldWorkEntry(BaseModel):\n",
        "        date: Optional[str] = Field(\n",
        "            None, description=\"Дата проведения операции (формат: 'мм-дд')\"\n",
        "        )\n",
        "        department_name: Literal[department_set] = Field(\n",
        "            ..., description=\"Название подразделения, в котором проводилась операция\"\n",
        "        )\n",
        "        operation: Literal[operation_set] = Field(\n",
        "            ..., description=\"Название выполненной операции\"\n",
        "        )\n",
        "        crop: Literal[crop_set] = Field(\n",
        "            ..., description=\"Культура, к которой относится операция\"\n",
        "        )\n",
        "        processed_area_day: int = Field(\n",
        "            ..., description=\"Обработанная площадь за день, в гектарах\"\n",
        "        )\n",
        "        processed_area_total: Optional[int] = Field(\n",
        "            None, description=\"Общая обработанная площадь с начала операции, в гектарах\"\n",
        "        )\n",
        "        yield_kg_day: Optional[int] = Field(\n",
        "            None, description=\"Валовая продукция за день, в килограммах\"\n",
        "        )\n",
        "        yield_kg_total: Optional[int] = Field(\n",
        "            None, description=\"Суммарная валовая продукция с начала операции, в килограммах\"\n",
        "        )\n",
        "\n",
        "        @field_validator('crop', mode='before')\n",
        "        @classmethod\n",
        "        def normalize_crop(cls, value):\n",
        "            if isinstance(value, str):\n",
        "                return value.replace('\\xa0', ' ').strip()\n",
        "            return value\n",
        "\n",
        "        # Added for cleaner comparison later\n",
        "        def model_dump_comparable(self) -> Dict[str, Any]:\n",
        "            \"\"\"Dumps model to dict, ensuring consistent handling of None/NaN.\"\"\"\n",
        "            dumped = self.model_dump() # Сначала получаем стандартный словарь модели\n",
        "            for key, value in dumped.items():\n",
        "                if is_empty(value): # Проверяем значение с помощью нашей функции is_empty\n",
        "                    dumped[key] = None # Если значение \"пустое\" (None или NaN), заменяем его на None\n",
        "            return dumped # Возвращаем словарь с нормализованными пустыми значениями\n",
        "\n",
        "\n",
        "    class FieldWorkLog(BaseModel):\n",
        "        entries: List[FieldWorkEntry] = Field(\n",
        "            ..., description=\"Список записей о полевых операциях\"\n",
        "        )\n",
        "\n",
        "    return FieldWorkEntry, FieldWorkLog"
      ],
      "metadata": {
        "id": "1JN_muiPlAbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FieldWorkEntry, FieldWorkLog = create_base_field_works_schemas(\n",
        "    department_set,\n",
        "    operation_set,\n",
        "    crop_set,\n",
        ")"
      ],
      "metadata": {
        "id": "Iy8HPFZoeN31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BASELINE"
      ],
      "metadata": {
        "id": "aPR9igh-_-FW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompts"
      ],
      "metadata": {
        "id": "eD7Iw59I10V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASELINE_SYSTEM_PROMPT_TEMPLATE = Template(PROMPTS_CONFIG.get(\"baseline_system_prompt_template\"))"
      ],
      "metadata": {
        "id": "7LYv9CC-14iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code\n",
        "\n",
        "Функции для генерации few-shot примеров из JSON файла"
      ],
      "metadata": {
        "id": "mEbQBcHH15hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_few_shot_text(few_shot_examples: List[Dict[str, Any]]) -> str:\n",
        "    blocks = []\n",
        "    for i, example in enumerate(few_shot_examples, start=1):\n",
        "        input_text = example[\"input\"]\n",
        "        output_block = example[\"output\"].model_dump_json(indent=4)\n",
        "        explanation = example.get(\"explanation\")\n",
        "\n",
        "\n",
        "        example_text = f\"\"\"# Example {i}:\n",
        "\n",
        "INPUT:\n",
        "<<<\n",
        "{input_text}\n",
        ">>>\n",
        "\n",
        "OUTPUT:\n",
        "<<<\n",
        "{output_block}\n",
        ">>>\"\"\"\n",
        "\n",
        "        if explanation:\n",
        "            example_text += f\"\"\"\n",
        "\n",
        "EXPLANATION:\n",
        "{explanation}\"\"\"\n",
        "\n",
        "        blocks.append(example_text)\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "\n",
        "def form_few_shot_str(json_path, chosen_examples):\n",
        "    \"\"\"\n",
        "    Формирует и возвращает в текстовом виде few-shot примеры.\n",
        "    \"\"\"\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    few_shot_data = []\n",
        "\n",
        "    for i, example in enumerate(data):\n",
        "        if i in chosen_examples:\n",
        "            input = example.get(\"message\")\n",
        "            explanation = example.get(\"explanation\")\n",
        "            # print(explanation)\n",
        "\n",
        "            output_block = FieldWorkLog(entries=[\n",
        "                    FieldWorkEntry(**entry) for entry in example.get(\"entries\")\n",
        "                ])\n",
        "\n",
        "            few_shot_data.append({\n",
        "                    \"input\": input,\n",
        "                    \"output\": output_block,\n",
        "                    \"explanation\": explanation\n",
        "                })\n",
        "\n",
        "    few_shot_examples_str = generate_few_shot_text(few_shot_data)\n",
        "\n",
        "    return few_shot_examples_str"
      ],
      "metadata": {
        "id": "XZTPxZDqHpDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model_on_structured_data_sync(\n",
        "    structured_data: List[dict],\n",
        "    excluded_ids: List[int],\n",
        "    model_output_schema: Type[BaseModel],\n",
        "    system_prompt_template,\n",
        "    model,\n",
        "    few_shot_examples_str: str,\n",
        "    schema_hints: str,\n",
        "    temperature: float = 0.1,\n",
        "    top_p: float = 1.0,\n",
        "    sleep_time: int = 0\n",
        ") -> List[dict]:\n",
        "    \"\"\"\n",
        "    Прогоняет LLM-модель по structured_data, исключая примеры из excluded_ids.\n",
        "    Позволяет управлять параметрами генерации (temperature, top_p).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    bound_model = model.with_structured_output(model_output_schema).bind(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "    for i, entry in enumerate(structured_data):\n",
        "        if i in excluded_ids:\n",
        "            continue\n",
        "\n",
        "        curr_message = entry['input']\n",
        "\n",
        "        system_prompt = system_prompt_template.render(\n",
        "            json_schema=json.dumps(model_output_schema.model_json_schema(), indent=2, ensure_ascii=False),\n",
        "            few_shot_examples=few_shot_examples_str,\n",
        "            message=curr_message,\n",
        "            schema_hints=schema_hints,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            answer = bound_model.invoke(\n",
        "                [SystemMessage(content=system_prompt)],\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Ошибка при обработке записи {i}: {e}\")\n",
        "            answer = FieldWorkLog(entries=[])\n",
        "\n",
        "        results.append({\n",
        "            \"input\": curr_message,\n",
        "            \"output\": answer\n",
        "        })\n",
        "\n",
        "        time.sleep(sleep_time) # для того чтобы не превышать rate limit\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "z7WwkRGzJT3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_model_on_structured_data_async(\n",
        "    structured_data: List[dict],\n",
        "    excluded_ids: List[int],\n",
        "    model_output_schema: Type[BaseModel],\n",
        "    system_prompt_template,\n",
        "    model,\n",
        "    few_shot_examples_str: str,\n",
        "    schema_hints: str,\n",
        "    max_concurrent_tasks: int = 5,\n",
        ") -> List[dict]:\n",
        "    \"\"\"\n",
        "    Асинхронно прогоняет LLM-модель по structured_data, исключая примеры из excluded_ids.\n",
        "    \"\"\"\n",
        "\n",
        "    semaphore = asyncio.Semaphore(max_concurrent_tasks)\n",
        "    results = []\n",
        "\n",
        "    bound_model = model.with_structured_output(model_output_schema)\n",
        "\n",
        "    async def process_entry(i, entry):\n",
        "        if i in excluded_ids:\n",
        "            return None\n",
        "\n",
        "        curr_message = entry['input']\n",
        "        system_prompt = system_prompt_template.render(\n",
        "            json_schema=json.dumps(model_output_schema.model_json_schema(), indent=2, ensure_ascii=False),\n",
        "            few_shot_examples=few_shot_examples_str,\n",
        "            message=curr_message,\n",
        "            schema_hints=schema_hints,\n",
        "        )\n",
        "\n",
        "        async with semaphore:\n",
        "            try:\n",
        "                answer = await bound_model.ainvoke(\n",
        "                    [SystemMessage(content=system_prompt)]\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Произошла ошибка при вызове модели: {e}\")\n",
        "                answer = FieldWorkLog(entries=[])\n",
        "            return {\n",
        "                \"input\": curr_message,\n",
        "                \"output\": answer\n",
        "            }\n",
        "\n",
        "    tasks = [process_entry(i, entry) for i, entry in enumerate(structured_data)]\n",
        "    raw_results = await asyncio.gather(*tasks)\n",
        "    results = [r for r in raw_results if r is not None]\n",
        "    return results"
      ],
      "metadata": {
        "id": "NnEdTmSdQDRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Генерация полного тестового датасета"
      ],
      "metadata": {
        "id": "00QihEJZwPZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference_on_xlsx(\n",
        "    xlsx_path: str,\n",
        "    model_output_schema: Type,\n",
        "    system_prompt_template,\n",
        "    model,\n",
        "    few_shot_examples_str: str,\n",
        "    schema_hints: str,\n",
        "    save_every: int = 10,\n",
        "    output_json_path: str = \"model_outputs.json\"\n",
        "):\n",
        "    # Загрузка входных сообщений\n",
        "    df = pd.read_excel(xlsx_path)\n",
        "    messages = df.iloc[:, 0].dropna().tolist()\n",
        "\n",
        "    # Подготовка данных в нужный формат\n",
        "    structured_data = [{\"agronomist_message\": msg} for msg in messages]\n",
        "\n",
        "    results = []\n",
        "    for i, entry in enumerate(tqdm(structured_data, desc=\"Processing messages\")):\n",
        "        prompt = system_prompt_template.render(\n",
        "            json_schema=model_output_schema.model_json_schema(),\n",
        "            few_shot_examples=few_shot_examples_str,\n",
        "            message=entry[\"agronomist_message\"],\n",
        "            schema_hints=schema_hints,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            answer = model.with_structured_output(model_output_schema).invoke(\n",
        "                [SystemMessage(content=prompt)]\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                \"input\": entry[\"agronomist_message\"],\n",
        "                \"output\": answer.model_dump()\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error on entry {i}: {e}\")\n",
        "            results.append({\n",
        "                \"input\": entry[\"agronomist_message\"],\n",
        "                \"output\": None,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "        # Сохраняем каждые `save_every` итераций\n",
        "        if (i + 1) % save_every == 0 or (i + 1) == len(structured_data):\n",
        "            with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"✅ Saved {len(results)} results to {output_json_path}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "SYLHVF3bUR7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "wrSKMqu222XO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Скрипт для автоматической проверки"
      ],
      "metadata": {
        "id": "YdQvQzfYoXqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подсчёт метрик и валидация ошибок на основе ground truth (эталонных) данных (изначально размечены моделью, затем провалидированы и поправлены вручную):\n",
        "- **field_accuracy** - средняя точность по всем полям\n",
        "- **exact_match_accuracy** - полное совпадение для ВСЕХ записей входного сообщения (если хотя бы одна запись несоответствует - для всего сообщения считается ошибкой)\n",
        "- **precision** , **f1**, **recall** - измеряют количество недостающих/лишних записей\n",
        "- **total_correctly_matched_entries** - количество записей которое на 100% совпало с ground_truth данными\n",
        "- **total_true_entries** - количество ground truth (эталонных) записей\n",
        "\n",
        "После выполнения, функция формирует результирующий JSON файл с подробными метриками и ошибками (конкретно какой пример и в чём ошибка)"
      ],
      "metadata": {
        "id": "TdADwpfqtaXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_empty(value):\n",
        "    return value is None or (isinstance(value, float) and math.isnan(value))\n",
        "\n",
        "def evaluate_predictions(ground_truth_data: List[Dict],\n",
        "                         predicted_data: List[Dict],\n",
        "                         excluded_ids: List[int],\n",
        "                         output_json_path: str = \"evaluation_results.json\"):\n",
        "    \"\"\"\n",
        "    Оценивает предсказания модели структурированных данных агронома.\n",
        "\n",
        "    Args:\n",
        "        ground_truth_data: Список словарей с эталонными данными ('message', 'entries').\n",
        "        predicted_data: Список словарей с предсказанными данными ('output': FieldWorkLog).\n",
        "        excluded_ids: Список индексов примеров для исключения из оценки.\n",
        "        output_json_path: Путь для сохранения JSON-файла с результатами.\n",
        "\n",
        "    Returns:\n",
        "        Словарь с общими метриками оценки.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        \"overall_metrics\": {\n",
        "            \"total_examples_evaluated\": 0,\n",
        "            \"field_accuracy\": 0.0,\n",
        "            \"exact_match_accuracy\": 0.0,\n",
        "            \"entry_level_metrics\": {\n",
        "                \"precision\": 0.0,\n",
        "                \"recall\": 0.0,\n",
        "                \"f1_score\": 0.0,\n",
        "                \"total_true_entries\": 0,\n",
        "                \"total_predicted_entries\": 0,\n",
        "                \"total_correctly_matched_entries\": 0,  # True Positives (TP) for entries\n",
        "                \"total_extra_entries\": 0,             # False Positives (FP) for entries\n",
        "                \"total_missing_entries\": 0            # False Negatives (FN) for entries\n",
        "            }\n",
        "        },\n",
        "        \"per_field_accuracy\": defaultdict(lambda: {\"correct\": 0, \"total\": 0, \"accuracy\": 0.0}),\n",
        "        \"mismatched_examples\": []\n",
        "    }\n",
        "\n",
        "    # Инициализация счетчиков для полей и метрик\n",
        "    total_fields_count = 0\n",
        "    correct_fields_count = 0\n",
        "    exact_matches_count = 0\n",
        "\n",
        "    total_true_entries = 0\n",
        "    total_pred_entries = 0\n",
        "    total_tp_entries = 0  # Matched entries based on content comparison (все поля)\n",
        "    total_fp_entries = 0  # Extra predicted entries\n",
        "    total_fn_entries = 0  # Missing ground truth entries\n",
        "\n",
        "    # Получаем список имен полей из Pydantic модели для инициализации счетчиков\n",
        "    field_names = list(FieldWorkEntry.model_fields.keys())\n",
        "    for field in field_names:\n",
        "        results[\"per_field_accuracy\"][field]  # Инициируем словари\n",
        "\n",
        "    # Фильтруем данные, исключая id\n",
        "    valid_indices = [i for i, _ in enumerate(ground_truth_data) if i not in excluded_ids]\n",
        "    if len(valid_indices) != len(predicted_data):\n",
        "        print(f\"Warning: Number of ground truth items ({len(valid_indices)}) after exclusion \"\n",
        "              f\"does not match number of predicted items ({len(predicted_data)}). \"\n",
        "              f\"Evaluation will proceed with {min(len(valid_indices), len(predicted_data))} items.\")\n",
        "        # Обрезаем до минимальной длины для zip\n",
        "        min_len = min(len(valid_indices), len(predicted_data))\n",
        "        valid_indices = valid_indices[:min_len]\n",
        "        predicted_data = predicted_data[:min_len]\n",
        "\n",
        "    results[\"overall_metrics\"][\"total_examples_evaluated\"] = len(valid_indices)\n",
        "\n",
        "    for i, pred_index in enumerate(valid_indices):\n",
        "        true_item = ground_truth_data[pred_index]\n",
        "        pred_item = predicted_data[i]  # уже синхронизировали длины\n",
        "\n",
        "        try:\n",
        "            true_entries_raw = true_item.get('entries', [])\n",
        "            true_log = FieldWorkLog(entries=[FieldWorkEntry(**entry) for entry in true_entries_raw])\n",
        "            pred_log = pred_item['output']\n",
        "            if not isinstance(pred_log, FieldWorkLog):\n",
        "                # Попробуем собрать FieldWorkLog из dict\n",
        "                if isinstance(pred_log, dict) and 'entries' in pred_log:\n",
        "                    try:\n",
        "                        pred_log = FieldWorkLog(**pred_log)\n",
        "                    except Exception as parse_error:\n",
        "                        print(f\"Error parsing predicted_data item {i} into FieldWorkLog: {parse_error}\")\n",
        "                        # Считаем, что предсказание пустое\n",
        "                        pred_log = FieldWorkLog(entries=[])\n",
        "                else:\n",
        "                    print(f\"Predicted data item {i}['output'] is not a FieldWorkLog instance or dict.\")\n",
        "                    pred_log = FieldWorkLog(entries=[])\n",
        "        except Exception as e:\n",
        "            # Ошибка парсинга, фиксируем mismatch и переходим к следующему\n",
        "            print(f\"Error processing item index {pred_index} (prediction index {i}): {e}\")\n",
        "            results[\"mismatched_examples\"].append({\n",
        "                \"example_index\": pred_index,\n",
        "                \"input_message\": true_item.get('message') or true_item.get(\"agronomist_message\", \"N/A\"),\n",
        "                \"status\": \"Parsing Error\",\n",
        "                \"error_details\": str(e),\n",
        "                \"expected_entries_raw\": true_item.get('entries', []),\n",
        "                \"predicted_output_raw\": pred_item.get('output', 'N/A')\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        true_entries = true_log.entries\n",
        "        pred_entries = pred_log.entries\n",
        "\n",
        "        n_true = len(true_entries)\n",
        "        n_pred = len(pred_entries)\n",
        "        total_true_entries += n_true\n",
        "        total_pred_entries += n_pred\n",
        "\n",
        "        # Будем фиксировать всю информацию о сравнениях в log_mismatch_details\n",
        "        log_mismatch_details = {\n",
        "            \"example_index\": pred_index,\n",
        "            \"input_message\": true_item.get('message') or true_item.get(\"agronomist_message\", \"N/A\"),\n",
        "            \"status\": \"Match\",  # переопределим ниже, если найдём расхождения\n",
        "            \"expected_entries\": [e.model_dump_comparable() for e in true_entries],\n",
        "            \"predicted_entries\": [e.model_dump_comparable() for e in pred_entries],\n",
        "            \"entry_comparison\": [],\n",
        "            \"entry_count_mismatch\": None\n",
        "        }\n",
        "\n",
        "        # По умолчанию считаем, что для \"точного совпадения\" лога нужно,\n",
        "        # чтобы кол-во записей совпадало и все поля в парах совпадали\n",
        "        is_exact_match_for_log = (n_true == n_pred and n_true > 0)\n",
        "\n",
        "        # --- Главная развилка: если кол-во записей совпадает, используем старую логику (порядок важен).\n",
        "        #     Иначе — «бес порядковое» сопоставление (bag-based).\n",
        "        if n_true == n_pred:\n",
        "            # --------------------- СТАРАЯ ЛОГИКА (порядок важен) ---------------------\n",
        "            if n_true == 0 and n_pred == 0:\n",
        "                # Если обе записи пустые, это тоже точное совпадение\n",
        "                exact_matches_count += 1\n",
        "            else:\n",
        "                matched_count_for_log = 0\n",
        "\n",
        "                for j, (true_entry, pred_entry) in enumerate(zip(true_entries, pred_entries)):\n",
        "                    true_dict = true_entry.model_dump_comparable()\n",
        "                    pred_dict = pred_entry.model_dump_comparable()\n",
        "                    entry_comparison_result = {\n",
        "                        \"entry_index\": j,\n",
        "                        \"status\": \"Match\",\n",
        "                        \"field_errors\": []\n",
        "                    }\n",
        "                    all_fields_match_in_entry = True\n",
        "\n",
        "                    for field_name in field_names:\n",
        "                        true_value = true_dict.get(field_name)\n",
        "                        pred_value = pred_dict.get(field_name)\n",
        "\n",
        "                        # Обновляем счётчик полей\n",
        "                        results[\"per_field_accuracy\"][field_name][\"total\"] += 1\n",
        "                        total_fields_count += 1\n",
        "\n",
        "                        # Сравниваем\n",
        "                        if true_value == pred_value or (is_empty(true_value) and is_empty(pred_value)):\n",
        "                            results[\"per_field_accuracy\"][field_name][\"correct\"] += 1\n",
        "                            correct_fields_count += 1\n",
        "                        else:\n",
        "                            all_fields_match_in_entry = False\n",
        "                            is_exact_match_for_log = False\n",
        "                            entry_comparison_result[\"status\"] = \"Field Mismatch\"\n",
        "                            entry_comparison_result[\"field_errors\"].append({\n",
        "                                \"field\": field_name,\n",
        "                                \"expected\": true_value,\n",
        "                                \"predicted\": pred_value\n",
        "                            })\n",
        "\n",
        "                    log_mismatch_details[\"entry_comparison\"].append(entry_comparison_result)\n",
        "                    if all_fields_match_in_entry:\n",
        "                        matched_count_for_log += 1\n",
        "\n",
        "                # TP = кол-во записей, где все поля совпали\n",
        "                tp_for_log = matched_count_for_log\n",
        "                # FP, FN = 0 в случае, если n_true == n_pred. Иначе ниже доначислим\n",
        "                fp_for_log = 0\n",
        "                fn_for_log = 0\n",
        "\n",
        "                total_tp_entries += tp_for_log\n",
        "                total_fp_entries += fp_for_log\n",
        "                total_fn_entries += fn_for_log\n",
        "\n",
        "                # Если все поля во всех записях совпали (и длины совпали), считаем exact match\n",
        "                if is_exact_match_for_log:\n",
        "                    exact_matches_count += 1\n",
        "\n",
        "                # Если нашлись несоответствия\n",
        "                if not is_exact_match_for_log:\n",
        "                    log_mismatch_details[\"status\"] = \"Field Mismatch\"\n",
        "                    results[\"mismatched_examples\"].append(log_mismatch_details)\n",
        "\n",
        "        else:\n",
        "            # --------------------- НОВАЯ ЛОГИКА (порядок не важен) ---------------------\n",
        "            log_mismatch_details[\"status\"] = \"Length Mismatch\"\n",
        "            log_mismatch_details[\"entry_count_mismatch\"] = {\n",
        "                \"expected_count\": n_true,\n",
        "                \"predicted_count\": n_pred\n",
        "            }\n",
        "\n",
        "            # 1) Составим списки индексов для GT (ground_truth) и Pred (предсказаний)\n",
        "            unmatched_true = list(range(n_true))\n",
        "            unmatched_pred = list(range(n_pred))\n",
        "            # Сопоставленные пары (gt_idx, pred_idx)\n",
        "            matched_pairs = []\n",
        "\n",
        "            # Функция для подсчёта кол-ва совпавших полей\n",
        "            def count_matching_fields(e1: FieldWorkEntry, e2: FieldWorkEntry) -> int:\n",
        "                d1 = e1.model_dump_comparable()\n",
        "                d2 = e2.model_dump_comparable()\n",
        "                count = 0\n",
        "                for fn in field_names:\n",
        "                    v1 = d1.get(fn)\n",
        "                    v2 = d2.get(fn)\n",
        "                    # Совпадение: либо равенство, либо оба \"пустые\"\n",
        "                    if v1 == v2 or (is_empty(v1) and is_empty(v2)):\n",
        "                        count += 1\n",
        "                return count\n",
        "\n",
        "            # 2) Сначала связываем полностью идентичные записи (все поля совпадают)\n",
        "            full_matches = []\n",
        "            to_remove_true = []\n",
        "            to_remove_pred = []\n",
        "            for gt_i in unmatched_true:\n",
        "                for pr_i in unmatched_pred:\n",
        "                    if count_matching_fields(true_entries[gt_i], pred_entries[pr_i]) == len(field_names):\n",
        "                        full_matches.append((gt_i, pr_i))\n",
        "                        to_remove_true.append(gt_i)\n",
        "                        to_remove_pred.append(pr_i)\n",
        "                        # Сразу же выходим из внутреннего цикла, т.к. один pred не может быть использован дважды\n",
        "                        break\n",
        "\n",
        "            # Удалим из списков эти совпавшие пары\n",
        "            for gt_i, pr_i in full_matches:\n",
        "                matched_pairs.append((gt_i, pr_i))\n",
        "            for x in to_remove_true:\n",
        "                if x in unmatched_true:\n",
        "                    unmatched_true.remove(x)\n",
        "            for x in to_remove_pred:\n",
        "                if x in unmatched_pred:\n",
        "                    unmatched_pred.remove(x)\n",
        "\n",
        "            # 3) Пока остались неиспользованные записи, сопоставляем самые \"похожие\" (по кол-ву совпавших полей)\n",
        "            while unmatched_true and unmatched_pred:\n",
        "                best_gt, best_pr = None, None\n",
        "                best_match_count = -1\n",
        "                for gt_i in unmatched_true:\n",
        "                    for pr_i in unmatched_pred:\n",
        "                        match_count = count_matching_fields(true_entries[gt_i], pred_entries[pr_i])\n",
        "                        if match_count > best_match_count:\n",
        "                            best_match_count = match_count\n",
        "                            best_gt = gt_i\n",
        "                            best_pr = pr_i\n",
        "\n",
        "                # Если самая похожая пара вовсе не совпадает ни по одному полю — смысла дальше нет\n",
        "                if best_match_count <= 0:\n",
        "                    break\n",
        "\n",
        "                # Иначе считаем эту пару сопоставленной\n",
        "                matched_pairs.append((best_gt, best_pr))\n",
        "                unmatched_true.remove(best_gt)\n",
        "                unmatched_pred.remove(best_pr)\n",
        "\n",
        "            # Далее формируем сравнение для каждой сопоставленной пары\n",
        "            matched_count_for_log = 0\n",
        "            for j, (gt_idx, pr_idx) in enumerate(matched_pairs):\n",
        "                true_entry = true_entries[gt_idx]\n",
        "                pred_entry = pred_entries[pr_idx]\n",
        "                true_dict = true_entry.model_dump_comparable()\n",
        "                pred_dict = pred_entry.model_dump_comparable()\n",
        "\n",
        "                entry_comparison_result = {\n",
        "                    \"entry_index\": j,  # индекс \"пары\" для отчёта\n",
        "                    \"status\": \"Match\",\n",
        "                    \"field_errors\": []\n",
        "                }\n",
        "                all_fields_match_in_entry = True\n",
        "\n",
        "                for field_name in field_names:\n",
        "                    true_value = true_dict.get(field_name)\n",
        "                    pred_value = pred_dict.get(field_name)\n",
        "\n",
        "                    # Обновляем счетчик общего количества полей\n",
        "                    results[\"per_field_accuracy\"][field_name][\"total\"] += 1\n",
        "                    total_fields_count += 1\n",
        "\n",
        "                    # Сравниваем значения\n",
        "                    if true_value == pred_value or (is_empty(true_value) and is_empty(pred_value)):\n",
        "                        results[\"per_field_accuracy\"][field_name][\"correct\"] += 1\n",
        "                        correct_fields_count += 1\n",
        "                    else:\n",
        "                        all_fields_match_in_entry = False\n",
        "                        entry_comparison_result[\"status\"] = \"Field Mismatch\"\n",
        "                        entry_comparison_result[\"field_errors\"].append({\n",
        "                            \"field\": field_name,\n",
        "                            \"expected\": true_value,\n",
        "                            \"predicted\": pred_value\n",
        "                        })\n",
        "\n",
        "                log_mismatch_details[\"entry_comparison\"].append(entry_comparison_result)\n",
        "                if all_fields_match_in_entry:\n",
        "                    matched_count_for_log += 1\n",
        "\n",
        "            # TP — это сколько пар совпало по всем полям\n",
        "            tp_for_log = matched_count_for_log\n",
        "            # FP — это оставшиеся unmatched_pred\n",
        "            fp_for_log = len(unmatched_pred)\n",
        "            # FN — это оставшиеся unmatched_true\n",
        "            fn_for_log = len(unmatched_true)\n",
        "\n",
        "            total_tp_entries += tp_for_log\n",
        "            total_fp_entries += fp_for_log\n",
        "            total_fn_entries += fn_for_log\n",
        "\n",
        "            # Отметим в отчёте \"Extra\" и \"Missing\" записи\n",
        "            for leftover_gt in unmatched_true:\n",
        "                log_mismatch_details[\"entry_comparison\"].append({\n",
        "                    \"entry_index\": leftover_gt,\n",
        "                    \"status\": \"Missing\",\n",
        "                    \"field_errors\": []\n",
        "                })\n",
        "            for leftover_pr in unmatched_pred:\n",
        "                log_mismatch_details[\"entry_comparison\"].append({\n",
        "                    \"entry_index\": leftover_pr,\n",
        "                    \"status\": \"Extra\",\n",
        "                    \"field_errors\": []\n",
        "                })\n",
        "\n",
        "            # Проверка на \"полное совпадение\": если все записи совпали и все поля внутри них совпали\n",
        "            # и их кол-во при этом в сумме совпадало — теоретически это редкая ситуация,\n",
        "            # ведь мы сюда попадаем только если n_true != n_pred. Но если вдруг...\n",
        "            if tp_for_log == n_true == n_pred:\n",
        "                # Это значит все записи идентичны\n",
        "                is_exact_match_for_log = True\n",
        "            else:\n",
        "                is_exact_match_for_log = False\n",
        "\n",
        "            if not is_exact_match_for_log:\n",
        "                # Если были расхождения, запишем в mismatched\n",
        "                # Статус уже \"Length Mismatch\", но если были расхождения в полях, пусть так и будет.\n",
        "                log_mismatch_details[\"status\"] = \"Length Mismatch\"  # или \"Field Mismatch\", но оставим как есть\n",
        "                results[\"mismatched_examples\"].append(log_mismatch_details)\n",
        "            else:\n",
        "                # Точное совпадение\n",
        "                exact_matches_count += 1\n",
        "\n",
        "        # --- конец развилки по кол-ву записей ---\n",
        "\n",
        "    # --- Расчёт итоговых метрик ---\n",
        "    num_evaluated = results[\"overall_metrics\"][\"total_examples_evaluated\"]\n",
        "\n",
        "    # Field Accuracy\n",
        "    if total_fields_count > 0:\n",
        "        results[\"overall_metrics\"][\"field_accuracy\"] = round((correct_fields_count / total_fields_count) * 100, 2)\n",
        "    else:\n",
        "        results[\"overall_metrics\"][\"field_accuracy\"] = 100.0 if num_evaluated > 0 else 0.0\n",
        "\n",
        "    # Exact Match Accuracy\n",
        "    if num_evaluated > 0:\n",
        "        results[\"overall_metrics\"][\"exact_match_accuracy\"] = round((exact_matches_count / num_evaluated) * 100, 2)\n",
        "\n",
        "    # Per-Field Accuracy\n",
        "    for field in field_names:\n",
        "        field_stats = results[\"per_field_accuracy\"][field]\n",
        "        if field_stats[\"total\"] > 0:\n",
        "            field_stats[\"accuracy\"] = round((field_stats[\"correct\"] / field_stats[\"total\"]) * 100, 2)\n",
        "        else:\n",
        "            field_stats[\"accuracy\"] = 100.0  # или \"N/A\"\n",
        "\n",
        "    # Entry Level Precision, Recall, F1\n",
        "    entry_metrics = results[\"overall_metrics\"][\"entry_level_metrics\"]\n",
        "    entry_metrics[\"total_true_entries\"] = total_true_entries\n",
        "    entry_metrics[\"total_predicted_entries\"] = total_pred_entries\n",
        "    entry_metrics[\"total_correctly_matched_entries\"] = total_tp_entries\n",
        "    entry_metrics[\"total_extra_entries\"] = total_fp_entries\n",
        "    entry_metrics[\"total_missing_entries\"] = total_fn_entries\n",
        "\n",
        "    # Precision = TP / (TP + FP)\n",
        "    precision_denom = total_tp_entries + total_fp_entries\n",
        "    if precision_denom > 0:\n",
        "        precision = total_tp_entries / precision_denom\n",
        "        entry_metrics[\"precision\"] = round(precision * 100, 2)\n",
        "    elif total_pred_entries == 0 and total_true_entries == 0:\n",
        "        entry_metrics[\"precision\"] = 100.0\n",
        "    else:\n",
        "        entry_metrics[\"precision\"] = 0.0\n",
        "\n",
        "    # Recall = TP / (TP + FN) == TP / total_true_entries\n",
        "    recall_denom = total_tp_entries + total_fn_entries\n",
        "    if recall_denom > 0:\n",
        "        recall = total_tp_entries / recall_denom\n",
        "        entry_metrics[\"recall\"] = round(recall * 100, 2)\n",
        "    elif total_true_entries == 0:\n",
        "        entry_metrics[\"recall\"] = 100.0\n",
        "    else:\n",
        "        entry_metrics[\"recall\"] = 0.0\n",
        "\n",
        "    # F1\n",
        "    precision_val = entry_metrics[\"precision\"] / 100\n",
        "    recall_val = entry_metrics[\"recall\"] / 100\n",
        "    if (precision_val + recall_val) > 0:\n",
        "        f1 = 2 * (precision_val * recall_val) / (precision_val + recall_val)\n",
        "        entry_metrics[\"f1_score\"] = round(f1 * 100, 2)\n",
        "    elif precision_val == 1.0 and recall_val == 1.0:\n",
        "        entry_metrics[\"f1_score\"] = 100.0\n",
        "    else:\n",
        "        entry_metrics[\"f1_score\"] = 0.0\n",
        "\n",
        "    # Сохраняем результаты в JSON\n",
        "    try:\n",
        "        # Преобразуем defaultdict в обычный dict для сериализации\n",
        "        results[\"per_field_accuracy\"] = dict(results[\"per_field_accuracy\"])\n",
        "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Результаты оценки сохранены в: {output_json_path}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Ошибка при сохранении JSON файла: {e}\")\n",
        "    except TypeError as e:\n",
        "        print(f\"Ошибка сериализации JSON: {e}. Проверьте типы данных в результатах.\")\n",
        "\n",
        "    return results[\"overall_metrics\"]"
      ],
      "metadata": {
        "id": "Jbk-SWunM4jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Подсказки по заполнению схемы"
      ],
      "metadata": {
        "id": "2CW8v86Uobm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema_hints = PROMPTS_CONFIG.get(\"baseline_schema_hints\")"
      ],
      "metadata": {
        "id": "YiYVqHNAhUxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_ground_truth_format(new_format_data: List[dict]) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Преобразует новый формат ground truth (input/output) в старый формат (message/entries).\n",
        "\n",
        "    Args:\n",
        "        new_format_data: список словарей с ключами 'input' и 'output'.\n",
        "\n",
        "    Returns:\n",
        "        Список словарей с ключами 'message' и 'entries'.\n",
        "    \"\"\"\n",
        "    converted = []\n",
        "    for item in new_format_data:\n",
        "        message = item.get(\"input\", \"\")\n",
        "        entries = item.get(\"output\", {}).get(\"entries\", [])\n",
        "        converted.append({\n",
        "            \"message\": message,\n",
        "            \"entries\": entries\n",
        "        })\n",
        "    return converted"
      ],
      "metadata": {
        "id": "EGmbl5GDaV_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_on_dataset(\n",
        "    important_data_dir: str,\n",
        "    main_dir: str,\n",
        "    model,\n",
        "    FieldWorkLog,\n",
        "    generate_few_shot_text,\n",
        "    run_model_on_structured_data_sync,\n",
        "    BASELINE_SYSTEM_PROMPT_TEMPLATE,\n",
        "    schema_hints,\n",
        "    evaluate_predictions,\n",
        "    convert_ground_truth_format,\n",
        "    test_filename: str = \"test_dataset.json\",\n",
        "    few_shot_filename: str = \"new_few_shots_auto_mode_data.json\",\n",
        "    output_filename: str = \"deepseek_v3_preza_evaluation_results.json\",\n",
        "    verbose: bool = True\n",
        "):\n",
        "    # Загрузка тестового датасета\n",
        "    test_dataset_json_path = os.path.join(important_data_dir, test_filename)\n",
        "    with open(test_dataset_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        ground_truth = json.load(f)\n",
        "\n",
        "    # Загрузка few-shot примеров\n",
        "    few_shot_json_path = os.path.join(important_data_dir, few_shot_filename)\n",
        "    with open(few_shot_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        few_shot_json_data = json.load(f)\n",
        "\n",
        "    # Преобразование output в сериализуемую структуру\n",
        "    serializable_results = [\n",
        "        {\n",
        "            \"input\": ex[\"input\"],\n",
        "            \"output\": FieldWorkLog(**ex[\"output\"]),\n",
        "            \"explanation\": ex.get(\"explanation\", \"\")\n",
        "        }\n",
        "        for ex in few_shot_json_data\n",
        "    ]\n",
        "\n",
        "    # Генерация текста для few-shot примеров\n",
        "    few_shot_examples_str = generate_few_shot_text(serializable_results)\n",
        "\n",
        "    # Запуск модели\n",
        "    results = run_model_on_structured_data_sync(\n",
        "        ground_truth,\n",
        "        [],\n",
        "        FieldWorkLog,\n",
        "        BASELINE_SYSTEM_PROMPT_TEMPLATE,\n",
        "        model,\n",
        "        few_shot_examples_str,\n",
        "        schema_hints,\n",
        "        # temperature=0.1,\n",
        "        # sleep_time=5,\n",
        "    )\n",
        "\n",
        "    # Сохранение и вычисление метрик\n",
        "    eval_save_path = os.path.join(main_dir, output_filename)\n",
        "    evaluation_results = evaluate_predictions(\n",
        "        convert_ground_truth_format(ground_truth),\n",
        "        results,\n",
        "        [],\n",
        "        eval_save_path\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nОбщие метрики:\")\n",
        "        print(json.dumps(evaluation_results, indent=4, ensure_ascii=False))\n",
        "\n",
        "    return evaluation_results"
      ],
      "metadata": {
        "id": "JSTCRPSCvD7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = evaluate_model_on_dataset(\n",
        "    important_data_dir=important_data_dir,\n",
        "    main_dir=main_dir,\n",
        "    model=model,\n",
        "    FieldWorkLog=FieldWorkLog,\n",
        "    generate_few_shot_text=generate_few_shot_text,\n",
        "    run_model_on_structured_data_sync=run_model_on_structured_data_sync,\n",
        "    BASELINE_SYSTEM_PROMPT_TEMPLATE=BASELINE_SYSTEM_PROMPT_TEMPLATE,\n",
        "    schema_hints=schema_hints,\n",
        "    evaluate_predictions=evaluate_predictions,\n",
        "    convert_ground_truth_format=convert_ground_truth_format\n",
        ")"
      ],
      "metadata": {
        "id": "SDdvpVTb5m_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0c409f-0f3a-4f6a-839d-d16d0f7ab5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты оценки сохранены в: drive/MyDrive/LLM_coding_challenge/deepseek_v3_preza_evaluation_results.json\n",
            "\n",
            "Общие метрики:\n",
            "{\n",
            "    \"total_examples_evaluated\": 106,\n",
            "    \"field_accuracy\": 99.9,\n",
            "    \"exact_match_accuracy\": 97.17,\n",
            "    \"entry_level_metrics\": {\n",
            "        \"precision\": 100.0,\n",
            "        \"recall\": 100.0,\n",
            "        \"f1_score\": 100.0,\n",
            "        \"total_true_entries\": 363,\n",
            "        \"total_predicted_entries\": 363,\n",
            "        \"total_correctly_matched_entries\": 360,\n",
            "        \"total_extra_entries\": 0,\n",
            "        \"total_missing_entries\": 0\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Определение спама\n",
        "Является ли пришедшее на вход сообщение записями агронома, которые нужно обработать или нет."
      ],
      "metadata": {
        "id": "eOYTpehld7Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPAM_FILTER_SYSTEM_PROMPT_TEMPLATE = Template(PROMPTS_CONFIG.get(\"spam_filter_system_prompt_template\"))\n",
        "spam_filter_few_shot_examples_str = generate_few_shot_text(PROMPTS_CONFIG.get(\"spam_filter_few_shot_examples_str\"))"
      ],
      "metadata": {
        "id": "iVQzVOq0eHdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Схема для Structured Output"
      ],
      "metadata": {
        "id": "FWLsMBW7wefS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MessageClassification(BaseModel):\n",
        "    explanation: str = Field(\n",
        "        ...,\n",
        "        description=\"Short explanation in Russian why the message should or should not be processed.\"\n",
        "    )\n",
        "    message_type: Literal[\"field_report\", \"non_report\"] = Field(\n",
        "        ...,\n",
        "        description=\"Message classification result: 'field_report' — if it's a pure fieldwork report, 'non_report' — if it's a question, discussion, planning, or irrelevant content.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "iAczNEBxl2sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Функция для классификации в асинхронном режиме"
      ],
      "metadata": {
        "id": "zt7uF4l0wvuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def classify_message_async(\n",
        "    message: str,\n",
        "    model_output_schema: Type,\n",
        "    few_shot_examples_classification,\n",
        "    system_prompt_template,\n",
        "    model,):\n",
        "    prompt = system_prompt_template.render(\n",
        "        json_schema=model_output_schema.model_json_schema(),\n",
        "        few_shot_examples_str=few_shot_examples_classification,\n",
        "        message=message)\n",
        "\n",
        "    answer = await model.with_structured_output(model_output_schema).ainvoke(\n",
        "        [SystemMessage(content=prompt)]\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "def classify_message_sync(\n",
        "    message: str,\n",
        "    model_output_schema: Type,\n",
        "    few_shot_examples_classification,\n",
        "    system_prompt_template,\n",
        "    model,):\n",
        "    prompt = system_prompt_template.render(\n",
        "        json_schema=model_output_schema.model_json_schema(),\n",
        "        few_shot_examples_str=few_shot_examples_classification,\n",
        "        message=message)\n",
        "\n",
        "    answer = model.with_structured_output(model_output_schema).invoke(\n",
        "        [SystemMessage(content=prompt)]\n",
        "    )\n",
        "    return answer"
      ],
      "metadata": {
        "id": "f9yWJ5SnCtuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Проверим на тестовых данных (в идеале не должно помечать как спам, потому что это действительные записи агрономов)"
      ],
      "metadata": {
        "id": "CxvdEuw5w8mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_messages(\n",
        "    dataset_path: str,\n",
        "    model,\n",
        "    MessageClassification,\n",
        "    classify_message_sync,\n",
        "    few_shot_examples_class_str: str,\n",
        "    system_prompt_template,\n",
        "    sleep_seconds: int = 5,\n",
        "    verbose: bool = True\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Классифицирует сообщения из JSON-файла и выводит сводку.\n",
        "\n",
        "    :param dataset_path: путь к JSON-файлу с сообщениями\n",
        "    :param model: модель для классификации\n",
        "    :param MessageClassification: Pydantic-схема результата\n",
        "    :param classify_message_sync: функция запуска модели\n",
        "    :param few_shot_examples_class_str: few-shot примеры в виде строки\n",
        "    :param system_prompt_template: шаблон системного промпта (Template)\n",
        "    :param sleep_seconds: пауза между запросами\n",
        "    :param verbose: выводить ли статистику после классификации\n",
        "    :return: список классифицированных результатов\n",
        "    \"\"\"\n",
        "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)\n",
        "\n",
        "    field_report_count = 0\n",
        "    non_report_count = 0\n",
        "    classified_results = []\n",
        "\n",
        "    for item in test_data:\n",
        "        message_text = item[\"input\"]\n",
        "\n",
        "        result = classify_message_sync(\n",
        "            message=message_text,\n",
        "            model_output_schema=MessageClassification,\n",
        "            few_shot_examples_classification=few_shot_examples_class_str,\n",
        "            system_prompt_template=system_prompt_template,\n",
        "            model=model,\n",
        "        )\n",
        "\n",
        "        classified_results.append({\n",
        "            \"input\": message_text,\n",
        "            \"classification\": result.message_type,\n",
        "            \"explanation\": result.explanation,\n",
        "        })\n",
        "\n",
        "        if result.message_type == \"field_report\":\n",
        "            field_report_count += 1\n",
        "        else:\n",
        "            non_report_count += 1\n",
        "\n",
        "        if sleep_seconds:\n",
        "            time.sleep(sleep_seconds)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n📊 Классификация завершена:\")\n",
        "        print(f\"  - Всего сообщений:         {len(test_data)}\")\n",
        "        print(f\"  - field_report:            {field_report_count}\")\n",
        "        print(f\"  - non_report (спам):       {non_report_count}\")\n",
        "\n",
        "    return classified_results"
      ],
      "metadata": {
        "id": "d0YQOtT0xHZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = classify_messages(\n",
        "    dataset_path=os.path.join(important_data_dir, \"test_dataset.json\"),\n",
        "    model=model,\n",
        "    MessageClassification=MessageClassification,\n",
        "    classify_message_sync=classify_message_sync,\n",
        "    few_shot_examples_class_str=few_shot_examples_class_str,\n",
        "    system_prompt_template=SPAM_FILTER_SYSTEM_PROMPT_TEMPLATE,\n",
        ")"
      ],
      "metadata": {
        "id": "2lXWhFE2CDsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1abd78c-03a0-46d1-ce44-b8edc91d73d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Классификация завершена:\n",
            "  - Всего сообщений:         106\n",
            "  - field_report:            106\n",
            "  - non_report (спам):       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Режим работы с обратной связью\n",
        "\n",
        "Модель может дать обратную связь в случае если посчитала что агроном неправильно отписал в сообщении"
      ],
      "metadata": {
        "id": "YZl9p5-eB5cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Схема для Structured Output"
      ],
      "metadata": {
        "id": "1yK9ElY9X19X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_annotated_field_work_log_schema(\n",
        "    department_set: Tuple[str],\n",
        "    operation_set: Tuple[str],\n",
        "    crop_set: Tuple[str],\n",
        "):\n",
        "    # --- Аннотированные типы для department_name ---\n",
        "    class DepartmentValid(BaseModel):\n",
        "        status: Literal['valid']\n",
        "        value: Literal[department_set] = Field(..., description=\"Корректное название подразделения\")\n",
        "\n",
        "    class DepartmentPredict(BaseModel):\n",
        "        status: Literal['predict']\n",
        "        value: Literal[department_set] = Field(..., description=\"Наиболее вероятное название подразделения\")\n",
        "        explanation: str = Field(..., description=\"Почему выбрано это значение\")\n",
        "\n",
        "    class DepartmentRaw(BaseModel):\n",
        "        status: Literal['raw']\n",
        "        value: str = Field(..., description=\"Произвольное значение подразделения\")\n",
        "        explanation: str = Field(..., description=\"Почему сохранено исходное значение\")\n",
        "\n",
        "    DepartmentNameAnnotated = Union[DepartmentValid, DepartmentPredict, DepartmentRaw]\n",
        "\n",
        "    # --- Аннотированные типы для operation ---\n",
        "    class OperationValid(BaseModel):\n",
        "        status: Literal['valid']\n",
        "        value: Literal[operation_set] = Field(..., description=\"Корректное название операции\")\n",
        "\n",
        "    class OperationPredict(BaseModel):\n",
        "        status: Literal['predict']\n",
        "        value: Literal[operation_set] = Field(..., description=\"Наиболее вероятное название операции\")\n",
        "        explanation: str = Field(..., description=\"Почему выбрано это значение\")\n",
        "\n",
        "    class OperationRaw(BaseModel):\n",
        "        status: Literal['raw']\n",
        "        value: str = Field(..., description=\"Произвольное значение операции\")\n",
        "        explanation: str = Field(..., description=\"Почему сохранено исходное значение\")\n",
        "\n",
        "    OperationAnnotated = Union[OperationValid, OperationPredict, OperationRaw]\n",
        "\n",
        "    # --- Аннотированные типы для crop ---\n",
        "    class CropValid(BaseModel):\n",
        "        status: Literal['valid']\n",
        "        value: Literal[crop_set] = Field(..., description=\"Корректное название культуры\")\n",
        "\n",
        "    class CropPredict(BaseModel):\n",
        "        status: Literal['predict']\n",
        "        value: Literal[crop_set] = Field(..., description=\"Наиболее вероятное название культуры\")\n",
        "        explanation: str = Field(..., description=\"Почему выбрано это значение\")\n",
        "\n",
        "    class CropRaw(BaseModel):\n",
        "        status: Literal['raw']\n",
        "        value: str = Field(..., description=\"Произвольное значение культуры\")\n",
        "        explanation: str = Field(..., description=\"Почему сохранено исходное значение\")\n",
        "\n",
        "    CropAnnotated = Union[CropValid, CropPredict, CropRaw]\n",
        "\n",
        "    # --- Основная запись ---\n",
        "    class FieldWorkEntryAnnotated(BaseModel):\n",
        "        date: Optional[str] = Field(\n",
        "            None, description=\"Дата проведения операции (формат: 'мм-дд')\"\n",
        "        )\n",
        "\n",
        "        department_name: DepartmentNameAnnotated = Field(..., description=\"Название подразделения с аннотацией\")\n",
        "        operation: OperationAnnotated = Field(..., description=\"Название операции с аннотацией\")\n",
        "        crop: CropAnnotated = Field(..., description=\"Название культуры с аннотацией\")\n",
        "\n",
        "        processed_area_day: int = Field(..., description=\"Обработанная площадь за день, в гектарах\")\n",
        "        processed_area_total: Optional[int] = Field(..., description=\"Общая обработанная площадь с начала операции, в гектарах\")\n",
        "        yield_kg_day: Optional[int] = Field(None, description=\"Валовая продукция за день, в килограммах\")\n",
        "        yield_kg_total: Optional[int] = Field(None, description=\"Суммарная валовая продукция с начала операции, в килограммах\")\n",
        "\n",
        "        def model_dump_comparable(self) -> Dict[str, Any]:\n",
        "            dumped = self.model_dump()\n",
        "            for key, value in dumped.items():\n",
        "                if value in [None, '', [], {}, float('nan')]:\n",
        "                    dumped[key] = None\n",
        "            return dumped\n",
        "\n",
        "    # --- Список записей ---\n",
        "    class FieldWorkLogAnnotated(BaseModel):\n",
        "        entries: List[FieldWorkEntryAnnotated] = Field(\n",
        "            ..., description=\"Список записей о полевых операциях с аннотированными полями\"\n",
        "        )\n",
        "\n",
        "    return FieldWorkEntryAnnotated, FieldWorkLogAnnotated"
      ],
      "metadata": {
        "id": "dGHu9c2rB8VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FieldWorkEntryAnnotated, FieldWorkLogAnnotated = create_annotated_field_work_log_schema(\n",
        "    department_set=department_set,\n",
        "    operation_set=operation_set,\n",
        "    crop_set=crop_set,\n",
        ")"
      ],
      "metadata": {
        "id": "b7j_XApaebUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### few shots"
      ],
      "metadata": {
        "id": "owLFIPJ3v_3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt"
      ],
      "metadata": {
        "id": "ga-GJjj-I1o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODE_DEMO_SYSTEM_PROMPT_TEMPLATE = Template(PROMPTS_CONFIG.get(\"mode_demo_system_prompt_template\"))"
      ],
      "metadata": {
        "id": "X8kLZxxRI4Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_mode_schema_hints = PROMPTS_CONFIG.get(\"demo_mode_schema_hints\")"
      ],
      "metadata": {
        "id": "5iVzjndtvE25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_annotated_to_strict(log_annotated: FieldWorkLogAnnotated) -> FieldWorkLog:\n",
        "    # Переводит формат из режима с обратной связью в строгий формат с чёткими полями\n",
        "    converted_entries: List[FieldWorkEntry] = []\n",
        "\n",
        "    for entry in log_annotated.entries:\n",
        "        # Если хотя бы одно из трёх полей помечено как raw — пропускаем\n",
        "        if (\n",
        "            entry.operation.status == \"raw\"\n",
        "            or entry.department_name.status == \"raw\"\n",
        "            or entry.crop.status == \"raw\"\n",
        "        ):\n",
        "            continue  # не включаем такую запись в результат\n",
        "\n",
        "        converted_entries.append(FieldWorkEntry(\n",
        "            date=entry.date,\n",
        "            department_name=entry.department_name.value,\n",
        "            operation=entry.operation.value,\n",
        "            crop=entry.crop.value,\n",
        "            processed_area_day=entry.processed_area_day,\n",
        "            processed_area_total=entry.processed_area_total,\n",
        "            yield_kg_day=entry.yield_kg_day,\n",
        "            yield_kg_total=entry.yield_kg_total\n",
        "        ))\n",
        "\n",
        "    return FieldWorkLog(entries=converted_entries)"
      ],
      "metadata": {
        "id": "E7rIzWnnyNDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_demo_mode_evaluation(\n",
        "    important_data_dir: str,\n",
        "    main_dir: str,\n",
        "    model,\n",
        "    FieldWorkLogAnnotated,\n",
        "    generate_few_shot_text: Callable,\n",
        "    run_model_on_structured_data: Callable,\n",
        "    MODE_DEMO_SYSTEM_PROMPT_TEMPLATE,\n",
        "    demo_mode_schema_hints: Dict,\n",
        "    convert_annotated_to_strict: Callable,\n",
        "    evaluate_predictions: Callable,\n",
        "    convert_ground_truth_format: Callable,\n",
        "    test_filename: str = \"test_dataset.json\",\n",
        "    few_shot_filename: str = \"emo_few_shot_examples.json\",\n",
        "    raw_output_filename: str = \"demo_mode_iter_2_evaluation_results.json\",\n",
        "    final_output_filename: str = \"demo_mode_iter_2_evaluation_results.json\",\n",
        "    temperature: float = 0.1,\n",
        "    sleep_time: int = 3,\n",
        "    verbose: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Запускает оценку модели в demo-режиме (annotated schema).\n",
        "\n",
        "    :return: словарь с метриками\n",
        "    \"\"\"\n",
        "    # Загружаем few-shot примеры\n",
        "    few_shot_path = os.path.join(important_data_dir, few_shot_filename)\n",
        "    with open(few_shot_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        few_shot_data = json.load(f)\n",
        "\n",
        "    for item in few_shot_data:\n",
        "        item[\"output\"] = FieldWorkLogAnnotated(**item[\"output\"])\n",
        "\n",
        "    few_shot_examples_str = generate_few_shot_text(few_shot_data)\n",
        "\n",
        "    # Загружаем тестовый датасет\n",
        "    test_path = os.path.join(important_data_dir, test_filename)\n",
        "    with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        ground_truth = json.load(f)\n",
        "\n",
        "    # Запускаем модель\n",
        "    results = run_model_on_structured_data(\n",
        "        ground_truth,\n",
        "        [],\n",
        "        FieldWorkLogAnnotated,\n",
        "        MODE_DEMO_SYSTEM_PROMPT_TEMPLATE,\n",
        "        model,\n",
        "        few_shot_examples_str,\n",
        "        demo_mode_schema_hints,\n",
        "        temperature=temperature,\n",
        "        sleep_time=sleep_time,\n",
        "    )\n",
        "\n",
        "    # Сохраняем \"сырые\" результаты\n",
        "    serializable_results = [\n",
        "        {\n",
        "            \"input\": res[\"input\"],\n",
        "            \"output\": res[\"output\"].model_dump(),\n",
        "            \"explanation\": res.get(\"explanation\", \"\")\n",
        "        }\n",
        "        for res in results\n",
        "    ]\n",
        "    raw_save_path = os.path.join(main_dir, raw_output_filename)\n",
        "    with open(raw_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(serializable_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Преобразуем результаты в strict-схему\n",
        "    for res in results:\n",
        "        res[\"output\"] = convert_annotated_to_strict(res[\"output\"])\n",
        "\n",
        "    # Оцениваем и сохраняем метрики\n",
        "    eval_save_path = os.path.join(main_dir, final_output_filename)\n",
        "    evaluation_results = evaluate_predictions(\n",
        "        convert_ground_truth_format(ground_truth),\n",
        "        results,\n",
        "        [],\n",
        "        eval_save_path\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nОбщие метрики:\")\n",
        "        print(json.dumps(evaluation_results, indent=4, ensure_ascii=False))\n",
        "\n",
        "    return evaluation_results"
      ],
      "metadata": {
        "id": "9QEcQ-8nrqRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = run_demo_mode_evaluation(\n",
        "    important_data_dir=important_data_dir,\n",
        "    main_dir=main_dir,\n",
        "    model=model,\n",
        "    FieldWorkLogAnnotated=FieldWorkLogAnnotated,\n",
        "    generate_few_shot_text=generate_few_shot_text,\n",
        "    run_model_on_structured_data=run_model_on_structured_data,\n",
        "    MODE_DEMO_SYSTEM_PROMPT_TEMPLATE=MODE_DEMO_SYSTEM_PROMPT_TEMPLATE,\n",
        "    demo_mode_schema_hints=demo_mode_schema_hints,\n",
        "    convert_annotated_to_strict=convert_annotated_to_strict,\n",
        "    evaluate_predictions=evaluate_predictions,\n",
        "    convert_ground_truth_format=convert_ground_truth_format,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5NufjHZDlzy",
        "outputId": "8b78ed01-61cb-4c1e-a392-7a684fd9595c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты оценки сохранены в: drive/MyDrive/LLM_coding_challenge/deepseek_demo_mode_iter_2_evaluation_results.json\n",
            "\n",
            "Общие метрики:\n",
            "{\n",
            "    \"total_examples_evaluated\": 106,\n",
            "    \"field_accuracy\": 99.72,\n",
            "    \"exact_match_accuracy\": 95.28,\n",
            "    \"entry_level_metrics\": {\n",
            "        \"precision\": 98.89,\n",
            "        \"recall\": 100.0,\n",
            "        \"f1_score\": 99.44,\n",
            "        \"total_true_entries\": 363,\n",
            "        \"total_predicted_entries\": 367,\n",
            "        \"total_correctly_matched_entries\": 355,\n",
            "        \"total_extra_entries\": 4,\n",
            "        \"total_missing_entries\": 0\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}